---
title: "Assignment 2"
author: "Michael Wu"
date: "16/09/2021"
output:
  html_document:
    css: style.css
---

# Part I: An analytical problem

```{r, warning=FALSE}
library(dplyr)
library(ggplot2)
library(quadprog)

X1 <- c(3, 4, 3.5, 5, 4, 6, 2, -1, 3, 3, -2, -1)
X2 <- c(2, 0, -1, 1, -3, -2, 5, 7, 6.5, 7, 7, 10)
Y <- factor(c(-1, -1, -1, -1, -1, -1, 1, 1, 1, 1, 1, 1))

data <- data_frame(X1, X2, Y) # create the dataframe required from the training data provided.
```

## Question 1:
Below is a scatter plot represent the points with Red colour for the class Y = 1 and Blue colour for Y = -1.
```{r}
# Plot data, where Red is for Y = 1 and Blue is for Y = -1
ggplot(data, aes(X2, X1, col=Y)) + 
  geom_point() +
  scale_color_manual(values = c("blue","red"))
```

## Question 2:
Hyperplane is a plane that separates two groups of samples, and an optimal hyperplane is that of the widest margin between the two groups of samples. Finding the optimal separating hyperplane is equivalent to finding a set of points which define the boundary between the two groups, and putting a wide band between those sets of points. According to the proposal of Vladimir Vapnik in 1963 ([Vapnik & Lerner, 1963](https://www.scirp.org/(S(351jmbntvnsjt1aadkposzje))/reference/ReferencesPapers.aspx?ReferenceID=1271416)), the maximum-margin hyperplane is defined as:

- w^T * x - b = 0

With the margins on both side defined as:

- w^T * x - b = 1 and w^T * x - b = -1, where w^T = transpose of w.

to separate the two groups of samples.

Therefore, we want to find w and b in the equations above so that the samples are correctly classified, such that w^T * x - b >= 1 for all the data with y = 1 and w^T * x - b <= -1 for all the data with y = -1. The distance between the two margins is defined as:

- ((1+b) - (-1+b)) / norm(w) => 2/norm(w)

and thus to make the margin as wide as possible, we minimise the norm(w). The constrained optimisation then becomes:

- minimum of norm(w), subject to w^T * x - b >= 1: y = 1 and w^T * x - b <= -1: y = -1. 

Hence, to solve this quadratic programming problem, and thus finding the optimal separating hyperplane, we used the function solve.QP() from the quadprog package in R, and sketch the optimal separting hyperplane in scatter plot. Refer to the R code below.

```{r}
# The quadprog package offers optimisation for problems with the form of:
# min(-d^T*b' + 1/2*b'^T*D*b') with the constraints A^T*b' >= b_0
# We then map our problem into the corresponding notation of the formula above, by setting the followings:
# d = 0
# b' = [w,b]
# D = identify matrix
# A^T = [yx1, yx2, ..., yxp, -y], where p = dimension of features
# b_0 = [1, ..., 1]

x <- data.frame(X2, X1) # combine the 2 features in a dataframe called x
y <- 2 * (as.numeric(Y) - 1.5) # convert Y into a numeric vector with the same length {-1, 1}
n <- length(Y) # number of observations
p <- ncol(x) # number of features
D <- matrix(0, nrow=p+1, ncol=p+1) # create a 3x3 matrix for D, this is because w is 1x2 vector (because of 2 features) and b is a 1x1 vector (i.e. a number) in the maximum-margin hyperplane formula above.
diag(D) <- 1 # set diagonal to 1, and make it an identity matrix.
D[p+1, p+1] <- 1e-8 # ensure D is positive definite matrix
d <- numeric(p+1) # create a 1x3 vector of 0
AT <- cbind(as.matrix(x), rep(-1,n)) # combine x with a column of -1, this is to create the -y part when dot product with the y vector below.
A <- t(AT*y) # dot product between x and y, and transpose.
b0 <- rep(1, n) # a vector of 1, as we set b_0 to 1.
wb <- solve.QP(D, d, A, b0) # store the result of the quadratic programming

w <- wb$solution[1:p]
b <- wb$solution[p+1]

ggplot(data, aes(X2, X1, col=Y)) + 
  geom_point() + 
  scale_color_manual(values = c("blue","red")) +
  geom_abline(intercept=(b+1)/w[2],slope=-w[1]/w[2],alpha=.2,linetype=2) +
  geom_abline(intercept=(b-1)/w[2],slope=-w[1]/w[2],alpha=.2,linetype=2) +
  geom_abline(intercept=b/w[2],slope=-w[1]/w[2],linetype=3)
```

## Question 3:
As mentioned above, the margins of the maximal margin classifer is defined as:

- w^T * x - b = 1 for y = 1
 
- w^T * x - b = -1 for y = -1

Therefore, the classification rule is:

- w^T * x - b >= 1 then y = 1, and

- w^T * x - b <= -1 then y = -1

Refer to the R code below to the test of this classification rule.

```{r}
t(as.matrix(w))%*%t(x[2,])-b
as.matrix(y)[2,]

t(as.matrix(w))%*%t(x[12,])-b
as.matrix(y)[12,]
```
## Question 4:
As mentioned above, if w^T * x - b >= 1 for all the data with y = 1 and w^T * x - b <= -1 for all the data with y = -1, then the distance between the two margins is defined as: 

- ((1+b) - (-1+b)) / norm(w) => 2/norm(w)

So to compute the margin (denoted as m) of the classifier, we simply caculate 2/norm(w), see code below.

```{r}
m <- 2/norm(as.matrix(w))
m
```

# Part II: An application
## 2.2.1 Data
```{r, warning=FALSE}
# select a random sample of 70% as train
library(caret)
cc_data <- readxl::read_xls('CreditCard_Data.xls', col_names = TRUE, skip = 1)

set.seed(1234)
split <- createDataPartition(cc_data$`default payment next month`, p = 0.7, list = FALSE) # 70% training, 30% test

train_set <- cc_data[split, ]
test_set <- cc_data[-split, ]

dim(train_set)
```

## 2.2.2 Tree Based Algorithms
### (a) Model Selection
For classifying credible and non-credible clients based on the dataset given, I have decided to use a Gradient Boosting algorithm with the xgboost package. This is because with a boosting algorithm, it improves the predictability of the tree-based methods by generating a large number of trees using bootstrapped samples and combine predictions of bootstrapped trees to achieve prediction stability. Boosting algorithm grows a tree sequentially and fit 'weak' classifiers to the original but modified data and combining weak classifiers to produce a strong committee. In other words, boosting algorithm learns from previous tree's mistake and improve on it in subsequent tree. I chose Gradient Boosting method over other boosting algorithms such as AdaBoost because Gradient Boosting is more robust and flexible than AdaBoost, as it can be utilised by any differentiable loss function, not just an exponential loss function in the case of AdaBoost.

In addition, we selected the following hyperparameters:

  + eta = 0.1 -> learning rate not too small (so that the model takes a long time to optimise), and not too big (so that the model bounce around and never converges)
  + max_depth = 3 -> maximum depth of a tree is 3 to avoid overfitting the model to the training set.
  + min_child_weight = 2 -> minimum number of observations required in each terminal node to be 2.
  
Also, the objective parameter of the xgboost is set to "binary:hinge", so it makes predictions of 0 or 1, rather than returning probabilities.

Refer to the R code below for model construction.
```{r, warning=FALSE}
library(xgboost)
predictor <- c("LIMIT_BAL", "SEX", "EDUCATION", "MARRIAGE", "AGE", "PAY_0", "PAY_2", "PAY_3", "PAY_4", "PAY_5", "PAY_6",
               "BILL_AMT1", "BILL_AMT2", "BILL_AMT3", "BILL_AMT4", "BILL_AMT5", "BILL_AMT6", "PAY_AMT1", "PAY_AMT2",
               "PAY_AMT3", "PAY_AMT4", "PAY_AMT5", "PAY_AMT6")
response <- c("default payment next month")

x_train <- train_set[, predictor] %>% as.matrix()
y_train <- train_set$`default payment next month`

### Gradient Boosting Model
# create params
params <- list(
  eta = 0.1, # learning rate
  max_depth = 3, # tree depth
  min_child_weight = 2 # minimum number of observations required in each terminal node
)

# train model
xgb_model <- xgboost(
  params = params,
  data = x_train,
  label = y_train,
  nrounds = 1000,
  objective = "binary:hinge",
  eval_metric = "rmse",
  verbose = 0
)
```

### (b) Model Summary - Variable Importance
```{r}
# create importance matrix
importance_matrix <- xgb.importance(model = xgb_model)

importance_matrix

# variable importance plot
xgb.plot.importance(importance_matrix, measure = "Gain")
```

The table and plot above displayed the model summary in terms of which features have the higher importance to the model. Based on the importance plot, we determined that feature PAY_0 (the repayment status in September 2005) has the highest importance to the model performance, second by PAY_AMT1 (the amount paid in September 2005). This makes sense as both of these features represent the repayment status and amount paid in the most recent month of the dataset, which often act as a good indicator of whether the customer will default or not in the next month after. Interestingly, features such as Education, Marriage, and Sex have the lowest importance to the model. This makes sense as by simply isolating these features, one cannot decide whether a customer is more or less likely to default, and thus neither can a machine learning model based on decision tree.

### (c) Model Performance on Training Set
```{r}
# predict using training data
pred_train_xgb <- predict(xgb_model, x_train)

# confusion matrix
confusionMatrix(factor(pred_train_xgb), factor(y_train))

```

Based on the confusion matrix on the training set, we determined that the model has a 0.8306 accuracy with the training data. 15639 out of 21000 (~74.5%) of the training sample are true positive, where the actual and predicted are both 0 (non-default payment); 1803 out of 21000 (~8.59%) of the training sample are true negative, where the actual and predicted are both 1 (default payment); 2869 out of 21000 (13.6%) of the training sample are false positive, where the actual is actually 1 (default payment) but predicted as 0 (non default payment); and lastly 689 out of 21000 (~3.28%) of the training sample are false negative, where the actual is 0 (non default payment) but predicted as 1 (default payment).

Furthermore, the sensitivity of the model is 0.9578, meaning that the model predicted the positive class (0) correctly ~95.36% of time, while the specificity of the model of 0.3859 indicating that the model may not be performing well when predicting the negative class (1).


## 2.2.3 Support vector classifier
### (a) Model Selection
There are 3 types of Support Vector Machines:

- Maximal margin classifier (linear decision boundary)

- Support vector classifiers (linear decision boundary, soft margin)

- Support vector machine (supports non-linear decision boundary)

In a real world dataset, it is very unlikely that a linear decision with hard boundary can be drawn between two groups of samples, and an amount of misclassification should be allowed. And thus, this ruled out implementing the Maximal margin classifier for the classification problem of credible and non-credible clients. Between the remaining two types of Support Vector Machines, I have selected the Support vector machine (non-linear decision boundary) for this problem. This is because being able to utilise the transformation kernels and enlarge the feature space allows for a better fit to the data points in classification problem, and hence should produce a more accurate model. While the soft margins with the Support vector classifiers allow a level of misclassification, in a two-class setting, the boundary between the two classes is linear for Support vector classifiers, and thus by using Support vector machine, we can capture the non-linearity of data in practice.

Refer to the R code below.

```{r}
# setting up parallel processing, as the svm model takes a long time to train, when tuneLength > 1.
library(parallel)
library(doParallel)

cluster <- makeCluster(detectCores() - 1) # leave 1 core for OS.
registerDoParallel(cluster)

# set up cross validation
train_control <- trainControl(method = "cv", number = 5, allowParallel = TRUE)

svm_model <- train(x = train_set[, predictor],
                   y = factor(train_set$`default payment next month`),
                   method = "svmRadial",
                   trControl = train_control,
                   preProcess = c("center", "scale"),
                   tuneLength = 6)

stopCluster(cluster)

svm_model
```
```{r}
# save svm model, so don't need to wait for the run everytime.
saveRDS(svm_model, "svm_model.rds")
svm_model <- readRDS("svm_model.rds")
```

### (b) Model Summary - Variable Importance


### (c) Model Performance on Training Set
```{r}
# predict using training data
pred_train_svm <- predict(svm_model, x_train)

# confusion matrix
confusionMatrix(factor(pred_train_svm), factor(y_train))

```

Based on the confusion matrix on the training set, we determined that the model has a 0.8307 accuracy with the training data, near identical to the gradient boosting model above. 15667 out of 21000 (~74.6%) of the training sample are true positive, where the actual and predicted are both 0 (non-default payment); 1778 out of 21000 (~8.47%) of the training sample are true negative, where the actual and predicted are both 1 (default payment); 2894 out of 21000 (13.8%) of the training sample are false positive, where the actual is actually 1 (default payment) but predicted as 0 (non default payment); and lastly 661 out of 21000 (~3.15%) of the training sample are false negative, where the actual is 0 (non default payment) but predicted as 1 (default payment).

Furthermore, the sensitivity of the model is 0.9595, meaning that the model predicted the positive class (0) correctly ~95.95% of time, while the specificity of the model of 0.3806 indicating that the model may not be performing well when predicting the negative class (1), slightly worse than the gradient boosting model.


## 2.2.4 Prediction
We will now apply the gradient boosting model and the support vector machine model on the test data to assess their respective performance. Refer to the R code below.

```{r}
# Predict on test data using xgboost model
x_test <- test_set[, predictor] %>% as.matrix()
y_test <- test_set$`default payment next month`

pred_test_xgb <- predict(xgb_model, x_test)
confusionMatrix(factor(pred_test_xgb), factor(y_test))
```
```{r}
# Predict on test data using svm model
pred_test_svm <- predict(svm_model, x_test)
confusionMatrix(factor(pred_test_svm), factor(y_test))
```

